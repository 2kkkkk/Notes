---
date: 2019-12-14 14:09
status: public
title: '9.hello world-Keras'
---

用mini-batch的原因：
1. 如果用随机梯度下降的话，需要对每一个样本单独计算LOSS，而如果是mini-batch的话，可以同时对多个样本计算LOSS，也就是可以写成矩阵相乘的形式，GPU在做矩阵运算时可以并行计算。
2. 如果用批梯度下降的话，那么缺少随机性，很容易迭代几次就掉到鞍点或者局部最小点。