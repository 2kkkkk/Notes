[TOC]

层次聚类类似于Hoffman树的构建。

# PCA

先说公式：$Z=WX$，$X$为原始数据集，维度为n$\times$m，n为样本数量，m为特征数量，$W$为由k个m维向量构成的矩阵（由m维降到k维），$Z$为降维后的数据，维度为n$\times$k。

怎样找到$W$呢， PCA的做法是：

1. 先找第一个m维向量w1，使数据集中的n个点投影到向量w1后的方差最大。
2. 再找第二个m维向量w2，在保证w1和w2正交的前提下，使数据集中的n个点投影到向量w2后的方差最大。
3. ......一直找到第k个m维向量wk，**每一个向量都要在保证与之前的向量正交的前提下**，使数据集中的n个点投影到向量w2后的方差最大。

怎样找到w1,w2,...,wk呢，以w1为例，
$$
Var(z_1)=\sum_{z_1}(z_1-\bar{z_1})^2=\sum_x(w^1\cdot (x-\bar{x}))^2=(w^1)^T\sum(x-\bar{x})(x-\bar{x})^Tw^1=(w^1)^TCov(x)w^1
$$
其中，$Cov(x)$为原始数据集$X$的协方差矩阵（**协方差矩阵为实对阵矩阵，其特征向量相互正交**），要使上式最大，通过拉格朗日乘子法，约束条件为$w^1(w^1)^T=1$（w的模为1，单位向量），可以求得w1为协方差矩阵$Cov(x)$对应于最大的特征值的特征向量，同理，可以求出w2...wk分别为对应于第2大...第k大的特征值的特征向量。

由于w1,w2,...,wk是相互正交的，因此降维后的数据$Z$的各特征列也是线性无关的（正交），且$Z$的协方差矩阵是对角矩阵。

## PCA和SVD 的区别与联系

 区别：PCA和SVD是两个完全不同的概念。PCA是主成分分析，是一种降维方式，通过选取k个主元即k个向量，w1,w2,...,wk，将数据集降维到k维。SVD是奇异值分解，是一种矩阵分解技术。

联系：PCA中的主成分是原始数据$X$的协方差矩阵$(x-\bar{x})(x-\bar{x})^T$的k个特征向量。矩阵$X$的SVD为$X=U\Sigma V^T$中的$U$是$XX^T$的k个特征向量。**因此如果将原始数据$X$中心化（各个特征列均值为0）得到$X^{'}$，对$X^{'} $做SVD，得到的$U^{'}$中的k个特征向量其实就是对原始数据$X$做PCA，得到的k个主元。**

 ## PCA的另一种角度

**通过PCA得到k个主元，数据集中每个样本可以看作是这k个主元的线性组合，每个样本线性组合的系数不同。**

## PCA的weakness

1. PCA是无监督的方式，高维空间中的两个类通过PCA降维后可能会被merge到一起，无法区分。LDA是有监督折方式。

2. PCA是线性的（各主元是正交的），如下图3维空间中的S形，PCA无法把它“拉直”，只能把它“拍扁”。

   ![1](https://github.com/2kkkkk/Notes/blob/master/algorithm/image/pca.jpg)

# 隐因子分解

以电影评分矩阵为例，矩阵中的某些列（每列代表一个电影）和某些行（每行代表一个用户）其实是线性相关的，因此可以进行隐因子分解。

用于NLP中，对文档和词的共现矩阵进行隐因子分解就是LSA。



# 附

PCA保留了高维 空间中的距离信息，如果两个点在高维空间中接近，那么降维后在低维空间中也是接近的。