---
date: 2019-12-15 11:44
status: public
title: 12.半监督
---

[TOC]

# 半监督+生成式模型

以二分类为例，

1. 先初始化参数：$\theta =\{ P(C_1),P(C_2), \mu ^{1},\mu ^{2},\Sigma \}$

2. 根据模型参数$\theta$计算无标签数据的后验概率$P_\theta(C_1|x^u)$

3. 根据第2步的结果，更新模型参数
   $$
   P(C_1)=\frac{N_1+\sum_{x^u}P(C_1|x^u))}{N}
   $$
   $$
   \mu ^1=\frac{1}{N_1}\sum_{x^{r}\epsilon C_1}x^r+\frac{1}{\sum_{x^u}P(C_1|x^u))}\sum_{x^u}P(C_1|x^u))x^u
   $$

公式2中的第2项相当于无标签数据的加权平均，权重为后验概率。

为什么要这样做呢？

在有监督+生成式模型中，我们通过极大化有标签数据的似然函数$logL(\theta)=\sum_{x^r}logP_\theta(x^r|\hat{y}^r)P(\hat{y}^r)$来求得模型参数，那么在半监督+生成式模型中，我们就要极大化有标签数据+无标签数据的似然函数$logL(\theta)=\sum_{x^r}logP_\theta(x^r|\hat{y}^r)P(\hat{y}^r)+\sum_{x^r}logP_\theta(x^u)$，由于不知道$x^u$从哪一个label中生成的，因此认为2个label都有可能，所以$x^u$出现的概率等于$P_\theta(x^u)=P_\theta(x^u|C_1)P(C_1)+P_\theta(x^u|C_2)P(C_2)$，但是这个式子不是convex的，因此要迭代的去解，也就是通过上面的3个步骤求解。

# 半监督+low density

上面的半监督+生成式模型中认为无标签数所按一定的后验概率属于不同的类别（soft label），而半监督+low density则认为“非黑即白”，即无标签数据只能属于某一特定类别（hard label），其做法是先用有标签数据训练得到一个模型$f^\*$，然后用该模型对无标签数据做预测，取部分预测的结果加回到有标签数据中重新训练，反复进行这个过程。**需要注意的是，如果是做回归任务，那么这个方法没用，因为加入的这些数据其实是由模型$f^\*$产生的，加入后重新训练得到的模型仍然是$f^\*$。**

对于神经网络来说，soft label这种方法没用，原因如上。

## Entropy-based regularization

一个分布的熵越大，则不确定性越高，可以将模型预测的无标签数据的概率分布的熵加入到损失函数中，即$L=\sum_{x^{r}}C(y^r,\hat{y}^r)+\lambda\sum_{x^{u}}E(y^u)$



# 半监督+smoothness assumption

思想：样本的分布不是均匀的，如果x1和x2在**一个高密度区域内**（可以想象成聚类后的x1和x2属于同一类）是接近的，那么$\hat{y}^1$和$\hat{y}^2$应该是一样的。

那么类似于Entropy-based regularization，我们也可以在损失函数中加入smooth regularization，即$L=\sum_{x^{r}}C(y^r,\hat{y}^r)+\lambda S$，其中$S=\frac{1}{2}\sum_{i,j}w_{i,j}(y^i-y^j)^2=\mathbf{y^T}L \mathbf{y}$，其中i,j为一个高密度区域内的两个点，$w_{i,j}$表示两个点的相似度similarity，L为拉普拉斯矩阵。