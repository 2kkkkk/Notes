介绍中说过，机器学习本质上就是找到一个函数$f(x)$，那么损失函数Loss function可以理解为函数$f(x)$的函数，它的输入为函数$f(x)$，输出为这个函数的好坏，可以表示为$L(f)$。

只要定义了Loss function且Loss function对参数是可微的，那么就可以用梯度下降法。

梯度下降法可以表示为：${w}'\leftarrow w_{0}-\eta \frac{\mathrm{d} L}{\mathrm{d} w}|_{w=w_{0}}$

以只有w,b两个参数的线性回归为例，损失函数的梯度为Loss function对所有参数的偏微分构成的vector，即
$$
\triangledown L=\begin{bmatrix}
\frac{\partial L}{\partial w}\\ 
\\
\frac{\partial L}{\partial b}
\end{bmatrix}
$$

梯度下降存在的问题：
- 对于非凸函数，可能得到局部最小
- 可能存在鞍点
- 可能存在平坦区域（梯度一直很小，接近于0）

---
对于$y=wx+b$这个线性模型来说，可以对模型进行修改，即换一个function set，可以修改为$y=w_1x^2+w_2x+b$或$y=w_1x^3+w_2x^2+w_3x+b$...，即加入x的2次项、3次项、4次项...实际上相当于提了新的feature，**随着feature越来越多，模型复杂度也会越来越高，因为对于线性模型来说，模型复杂度即参数个数，也就是特征个数；但树模型的模型复杂度不会随着特征数量增多而增加，因为树模型的模型复杂度是树的深度、叶子节点个数这些**。当模型复杂度过高时，训练集上的error会很低，会测试集的error会很高，即过拟合，**过拟合其实就是模型学习到了这份训练集上“独有”的一些参数，就像科目二考试中看到后轮压到白线后就打满方向盘一样。**

解决过拟合的一个方法就是，扩大训练集，相当于换了个科目二场地（之前的“规律”可能就没用了）。

另一个方法是修改Loss function，在原先的损失函数中加入正则项，使模型平滑。

---
课中提到，如果不考虑宝可梦的种类，就对其CP值进行预测，是在瞎忙，因为不同类别的宝可梦，其进化模式是不同的。**因为对于实际的业务场景来说，也要具体问题具体分析，根据情况建立不同的模型，例如嫌疑人预测时要区分犯罪类型。**

---
模型效果差可能的原因：
- 预测的量本身就有随机性
- 还有其他隐藏因素的影响（我的理解是可以多提feature来优化）